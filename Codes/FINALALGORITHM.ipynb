{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e4950270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'software':\n",
      "computer: 0.9748510122299194\n",
      "search: 0.9716047644615173\n",
      "internet: 0.9667606353759766\n",
      "access: 0.9660167694091797\n",
      "user: 0.9633614420890808\n",
      "\n",
      "Most similar word: computer (Similarity: 0.9748510122299194)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def find_similar_words(input_word, model, topn=5):\n",
    "    \"\"\"\n",
    "    Find and print similar words to the input word using a pre-trained Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_word (str): The input word for which to find similar words.\n",
    "    - model (Word2Vec): The pre-trained Word2Vec model.\n",
    "    - topn (int): Number of similar words to retrieve (default is 5).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find similar words to the input word\n",
    "        similar_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Similar words to '{input_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"{word}: {similarity}\")\n",
    "\n",
    "        # Find the most similar word and its similarity score\n",
    "        most_similar_word, max_similarity = max(similar_words, key=lambda x: x[1])\n",
    "        print(f\"\\nMost similar word: {most_similar_word} (Similarity: {max_similarity})\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Word '{input_word}' not found in the vocabulary.\")\n",
    "\n",
    "# Example usage:\n",
    "# Load your pre-trained Word2Vec model (replace 'your_model_path' with the actual path to your model file)\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Input word for which to find similar words\n",
    "input_word = \"software\"\n",
    "\n",
    "# Call the function\n",
    "find_similar_words(input_word, word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542d2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Topic Distributions:\n",
      "Topic 1: {'search': 0.2, 'software': 0.2, 'information': 0.2, 'internet': 0.2, 'computer': 0.2}\n",
      "\n",
      "Original Text:\n",
      "digital technology\n",
      "\n",
      "Sampled Text with Revealed Topics:\n",
      "internet computer\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Function to find similar words\n",
    "def find_similar_words(input_word, model, topn=5):\n",
    "    try:\n",
    "        # Find similar words to the input word\n",
    "        similar_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
    "        return [word for word, _ in similar_words]\n",
    "    except KeyError:\n",
    "        print(f\"Word '{input_word}' not found in the vocabulary.\")\n",
    "        return []\n",
    "\n",
    "# Function to extract topic distributions\n",
    "def extract_topic_distributions(model, input_text, num_topics=1):\n",
    "    topic_distributions = []\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_tokens = preprocess_text(input_text)\n",
    "\n",
    "    for _ in range(num_topics):\n",
    "        # Choose a random word from the input text\n",
    "        random_word = np.random.choice(input_tokens)\n",
    "\n",
    "        # Find similar words to the random word\n",
    "        similar_words = find_similar_words(random_word, model)\n",
    "\n",
    "        # Normalize the distribution\n",
    "        topic_distribution = {word: 1.0 / len(similar_words) for word in similar_words}\n",
    "        topic_distributions.append(topic_distribution)\n",
    "\n",
    "    return topic_distributions\n",
    "\n",
    "# Perform Gibbs sampling to reveal hidden topics\n",
    "def gibbs_sampling(text, model, topic_distributions, num_iterations=100):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    num_topics = len(topic_distributions)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        for i, word in enumerate(tokenized_text):\n",
    "            if word in model.wv.key_to_index and random.random() < 0.5:\n",
    "                sampled_topic = np.random.choice(num_topics)\n",
    "                sampled_word = np.random.choice(list(topic_distributions[sampled_topic].keys()), p=list(topic_distributions[sampled_topic].values()))\n",
    "                tokenized_text[i] = sampled_word\n",
    "\n",
    "    return ' '.join(tokenized_text)\n",
    "\n",
    "# Example usage:\n",
    "# Load your pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Input text for which to extract topic distributions\n",
    "input_text = \"digital technology\"\n",
    "\n",
    "# Call the function to extract topic distributions\n",
    "topic_distributions = extract_topic_distributions(word2vec_model, input_text)\n",
    "\n",
    "# Print the extracted topic distributions\n",
    "print(\"Extracted Topic Distributions:\")\n",
    "for i, topic_distribution in enumerate(topic_distributions, 1):\n",
    "    print(f\"Topic {i}: {topic_distribution}\")\n",
    "\n",
    "# Perform Gibbs sampling to reveal hidden topics\n",
    "sampled_text = gibbs_sampling(input_text, word2vec_model, topic_distributions)\n",
    "\n",
    "# Print original and sampled text\n",
    "print(\"\\nOriginal Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nSampled Text with Revealed Topics:\")\n",
    "print(sampled_text)\n",
    "\n",
    "# Calculate Coherence Score (C_v)\n",
    "#texts = [preprocess_text(sampled_text)]  # Assuming you want to calculate coherence for the sampled text\n",
    "#dictionary = corpora.Dictionary(texts)\n",
    "#corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#coherence_model = CoherenceModel(topics=[list(topic.keys()) for topic in topic_distributions], texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "#print(f\"\\nC_v Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "585f2d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.020024076), (1, 0.4085462), (2, 0.5305317), (3, 0.020530822), (4, 0.020367196)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import models\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Load your pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Load the BBC News train dataset \n",
    "dataset_path = 'bbc News Train.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the text (tokenize)\n",
    "texts = [word_tokenize(text.lower()) for text in df['Text']]\n",
    "\n",
    "# Create a Gensim dictionary from the Word2Vec vocabulary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a corpus of bag-of-words vectors\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the LDA model\n",
    "train_lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                   id2word=dictionary,\n",
    "                                   num_topics=5,  # Specify the number of topics\n",
    "                                   passes=20)   # Number of training iterations\n",
    "# Assuming lda_model is your trained LDA model\n",
    "train_lda_model.save(\"train_lda_model\")\n",
    "# Print the topics identified by the model\n",
    "#print(lda_model.print_topics())\n",
    "\n",
    "# Analyze a new document\n",
    "new_doc = \"I enjoy playing sports and listening to music.\"\n",
    "new_doc_bow = dictionary.doc2bow(word_tokenize(new_doc.lower()))\n",
    "print(train_lda_model.get_document_topics(new_doc_bow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f14ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted LDA Topic Distributions:\n",
      "Topic 1: {'.': 8.848936e-05, 'the': 7.940277e-05, 'to': 6.110881e-05, 'of': 5.836123e-05, 'in': 5.4996082e-05, 'and': 5.475499e-05, 'a': 4.9052138e-05, 'on': 4.844017e-05, 'for': 4.8261707e-05, 'is': 4.822316e-05}\n",
      "Topic 2: {'the': 0.05383811, '.': 0.043967824, 'to': 0.022821352, 'and': 0.02087702, 'a': 0.020793112, 'in': 0.01971501, 'of': 0.015657177, 's': 0.012384042, 'for': 0.01030647, 'i': 0.009409825}\n",
      "\n",
      "Text:\n",
      "Technology software\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, CoherenceModel\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Function to find similar words\n",
    "def find_similar_words(input_word, model, topn=5):\n",
    "    try:\n",
    "        # Find similar words to the input word\n",
    "        similar_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
    "        return [word for word, _ in similar_words]\n",
    "    except KeyError:\n",
    "        print(f\"Word '{input_word}' not found in the vocabulary.\")\n",
    "        return []\n",
    "\n",
    "# Function to extract topic distributions using LDA\n",
    "def extract_lda_topic_distributions(text, num_topics=2):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    dictionary = corpora.Dictionary([tokenized_text])\n",
    "    corpus = [dictionary.doc2bow(tokenized_text)]\n",
    "\n",
    "    #lda_model = LdaModel(corpus, num_topics=num_topics)\n",
    "    topic_distributions = [{word: probability for word, probability in lda_model.show_topic(topic)} for topic in range(num_topics)]\n",
    "\n",
    "    return lda_model, topic_distributions\n",
    "\n",
    "# Perform Gibbs sampling to reveal hidden topics using LDA\n",
    "def gibbs_sampling(text, word2vec_model, lda_model, topic_distributions, num_iterations=100):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    num_topics = len(topic_distributions)\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    dictionary = corpora.Dictionary([tokenized_text])\n",
    "    corpus = [dictionary.doc2bow(tokenized_text)]\n",
    "    for _ in range(num_iterations):\n",
    "        for i, word in enumerate(tokenized_text):\n",
    "            if word in word2vec_model.wv.key_to_index and random.random() < 0.5:\n",
    "                # Sample a topic using LDA's word-topic distribution\n",
    "                topic_distribution = lda_model.get_document_topics(dictionary.doc2bow(tokenized_text))[0]\n",
    "                normalized_probs = np.array(topic_distribution) / np.sum(topic_distribution)\n",
    "                sampled_topic = np.random.choice(num_topics, p=normalized_probs)\n",
    "\n",
    "                # Sample a word from the chosen topic distribution\n",
    "                #sampled_word = np.random.choice(list(topic_distributions[sampled_topic].keys()),\n",
    "                 #                               p=list(topic_distributions[sampled_topic].values()))\n",
    "                topic_probs = list(topic_distributions[sampled_topic].values())\n",
    "                normalized_probs = np.array(topic_probs) / np.sum(topic_probs)\n",
    "                sampled_word = np.random.choice(list(topic_distributions[sampled_topic].keys()), p=normalized_probs)\n",
    "\n",
    "                tokenized_text[i] = sampled_word\n",
    "\n",
    "    return ' '.join(tokenized_text)\n",
    "\n",
    "# Example usage:\n",
    "# Load your pre-trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Input text for which to extract LDA topic distributions\n",
    "input_text = \"Technology software\"\n",
    "\n",
    "# Call the function to extract LDA topic distributions\n",
    "lda_model, lda_topic_distributions = extract_lda_topic_distributions(input_text)\n",
    "\n",
    "# Print the extracted LDA topic distributions\n",
    "print(\"Extracted LDA Topic Distributions:\")\n",
    "for i, topic_distribution in enumerate(lda_topic_distributions, 1):\n",
    "    print(f\"Topic {i}: {topic_distribution}\")\n",
    "\n",
    "# Perform Gibbs sampling to reveal hidden topics using LDA\n",
    "sampled_text_lda = gibbs_sampling(input_text, word2vec_model, lda_model, lda_topic_distributions)\n",
    "\n",
    "# Print original and sampled text\n",
    "print(\"\\nText:\")\n",
    "print(input_text)\n",
    "\n",
    "# Calculate Coherence Score (C_v)\n",
    "#texts_lda = [preprocess_text(sampled_text_lda)]\n",
    "#dictionary_lda = corpora.Dictionary(texts_lda)\n",
    "#corpus_lda = [dictionary_lda.doc2bow(text) for text in texts_lda]\n",
    "\n",
    "#coherence_model_lda = CoherenceModel(topics=[list(topic.keys()) for topic in lda_topic_distributions], texts=texts_lda, dictionary=dictionary_lda, coherence='c_v')\n",
    "#coherence_score_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "#print(f\"\\nC_v Coherence Score using LDA: {coherence_score_lda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1fa3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'.': 8.848936e-05, 'the': 7.940277e-05, 'to': 6.110881e-05, 'of': 5.836123e-05, 'in': 5.4996082e-05, 'and': 5.475499e-05, 'a': 4.9052138e-05, 'on': 4.844017e-05, 'for': 4.8261707e-05, 'is': 4.822316e-05}, {'the': 0.05383811, '.': 0.043967824, 'to': 0.022821352, 'and': 0.02087702, 'a': 0.020793112, 'in': 0.01971501, 'of': 0.015657177, 's': 0.012384042, 'for': 0.01030647, 'i': 0.009409825}, {'the': 0.051936768, '.': 0.041534677, 'to': 0.028651876, 'of': 0.024257185, 'and': 0.01992346, 'a': 0.018725405, 'in': 0.018152766, 'is': 0.010437443, 'that': 0.010417253, 'it': 0.009499688}, {'the': 0.046005614, '.': 0.03853317, 'to': 0.024643684, 'of': 0.021529086, 'and': 0.020040892, 'a': 0.019108659, 'in': 0.017469147, 'is': 0.009071474, 'that': 0.00905134, 'mr': 0.008966147}]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract topic distributions using LDA\n",
    "def topic_distributions(text, num_topics=4):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    dictionary = corpora.Dictionary([tokenized_text])\n",
    "    corpus = [dictionary.doc2bow(tokenized_text)]\n",
    "\n",
    "    #lda_model = LdaModel(corpus, num_topics=num_topics)\n",
    "    topic_distributions = [{word: probability for word, probability in train_lda_model.show_topic(topic)} for topic in range(num_topics)]\n",
    "\n",
    "    return topic_distributions\n",
    "new_doc = \"I enjoy playing sports and listening to music.\"\n",
    "new_doc_bow = dictionary.doc2bow(word_tokenize(new_doc.lower()))\n",
    "#print(train_lda_model.get_document_topics(new_doc_bow))\n",
    "\n",
    "lda_topic_distributions = topic_distributions(new_doc)\n",
    "doc_word_matrix = lda_topic_distributions\n",
    "print(lda_topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fd7ddcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Topic Distributions: [(0, 0.033351753), (1, 0.034304705), (2, 0.57570213), (3, 0.3227982), (4, 0.03384328)]\n",
      "Sampled Topic: 3\n",
      "Nearest Word: its\n",
      "Sampled Topic: 2\n",
      "Nearest Word: aggression\n",
      "Sampled Topic: 4\n",
      "Nearest Word: would\n",
      "\n",
      "Final Sampled Text:\n",
      "its aggression science is would\n"
     ]
    }
   ],
   "source": [
    "def gibbs_sampling(text, lda_model, word2vec_model, nn, lambda_, num_iterations=1):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration+1}:\")\n",
    "\n",
    "        # Estimate topic distributions\n",
    "        topic_distributions = lda_model.get_document_topics(\n",
    "            corpora.Dictionary([tokenized_text]).doc2bow(tokenized_text)\n",
    "        )\n",
    "        print(\"Topic Distributions:\", topic_distributions)\n",
    "\n",
    "        # Extract probabilities and normalize\n",
    "        probabilities = np.array([topic[1] for topic in topic_distributions])\n",
    "        normalized_probs = probabilities / np.sum(probabilities)\n",
    "\n",
    "        for i, word in enumerate(tokenized_text):\n",
    "            if word in word2vec_model.wv.key_to_index and random.random() < lambda_:\n",
    "                # Sample a topic\n",
    "                sampled_topic = np.random.choice(num_topics, p=normalized_probs)\n",
    "                print(f\"Sampled Topic: {sampled_topic}\")\n",
    "                # Sample a word vector\n",
    "                p = [prob for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = [prob / np.sum(p) for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = np.asarray(p).astype('float64')\n",
    "                p = p / np.sum(p)\n",
    "                sampled_word_vector = word2vec_model.wv[np.random.choice(word2vec_model.wv.index_to_key, p=p)]\n",
    "\n",
    "                # Find the nearest word\n",
    "                nearest_word_index = nn.kneighbors(sampled_word_vector.reshape(1, -1), return_distance=False)[0][0]\n",
    "                \n",
    "                # Check if the nearest word is an actual word\n",
    "                if nearest_word_index < len(word2vec_model.wv.index_to_key):\n",
    "                    nearest_word = word2vec_model.wv.index_to_key[nearest_word_index]\n",
    "                    print(f\"Nearest Word: {nearest_word}\")\n",
    "\n",
    "                    # Update counts (implementation depends on your LDA model)\n",
    "                    # lda_model.update_counts(tokenized_text, i, sampled_topic)\n",
    "\n",
    "                    # Replace the original word\n",
    "                    tokenized_text[i] = nearest_word\n",
    "\n",
    "    print(\"\\nFinal Sampled Text:\")\n",
    "    # Convert elements in tokenized_text to strings before joining\n",
    "    tokenized_text_str = [str(token) for token in tokenized_text]\n",
    "    return ' '.join(tokenized_text_str)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"education of science is great\"\n",
    "sampled_text = gibbs_sampling(input_text, lda_model, word2vec_model, nn, lambda_=0.5)\n",
    "print(sampled_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c8b5cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Topic Distributions: [(0, 0.033351745), (1, 0.034304738), (2, 0.57568455), (3, 0.32281566), (4, 0.033843286)]\n",
      "Sampled Topic: 2\n",
      "Nearest Word: in\n",
      "Sampled Topic: 2\n",
      "Nearest Word: of\n",
      "Sampled Topic: 2\n",
      "Nearest Word: microsoft\n",
      "Iteration 2:\n",
      "Topic Distributions: [(0, 0.03335176), (1, 0.03430451), (2, 0.57580173), (3, 0.32269874), (4, 0.033843216)]\n",
      "Sampled Topic: 3\n",
      "Nearest Word: actor\n",
      "\n",
      "Final Sampled Text:\n",
      "software actor technology of microsoft\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import LdaModel, Word2Vec\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load pre-trained models (replace with actual paths)\n",
    "lda_model = train_lda_model\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Initialize LSH or k-d tree\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "nn.fit(word2vec_model.wv.vectors)\n",
    "\n",
    "def gibbs_sampling(text, lda_model, word2vec_model, nn, lambda_, num_iterations=2):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration+1}:\")\n",
    "\n",
    "        # Estimate topic distributions\n",
    "        topic_distributions = lda_model.get_document_topics(\n",
    "            corpora.Dictionary([tokenized_text]).doc2bow(tokenized_text)\n",
    "        )\n",
    "        print(\"Topic Distributions:\", topic_distributions)\n",
    "\n",
    "        # Extract probabilities and normalize\n",
    "        probabilities = np.array([topic[1] for topic in topic_distributions])\n",
    "        normalized_probs = probabilities / np.sum(probabilities)\n",
    "\n",
    "        for i, word in enumerate(tokenized_text):\n",
    "            if word in word2vec_model.wv.key_to_index and random.random() < lambda_:\n",
    "                # Sample a topic\n",
    "                sampled_topic = np.random.choice(num_topics, p=normalized_probs)\n",
    "                print(f\"Sampled Topic: {sampled_topic}\")\n",
    "                # Sample a word vector\n",
    "                p = [prob for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = [prob / np.sum(p) for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = np.asarray(p).astype('float64')\n",
    "                p = p / np.sum(p)\n",
    "                sampled_word_vector = word2vec_model.wv[np.random.choice(word2vec_model.wv.index_to_key, p=p)]\n",
    "\n",
    "                # Find the nearest word\n",
    "                nearest_word_index = nn.kneighbors(sampled_word_vector.reshape(1, -1), return_distance=False)[0][0]\n",
    "                \n",
    "                # Check if the nearest word is an actual word\n",
    "                if nearest_word_index < len(word2vec_model.wv.index_to_key):\n",
    "                    nearest_word = word2vec_model.wv.index_to_key[nearest_word_index]\n",
    "                    print(f\"Nearest Word: {nearest_word}\")\n",
    "\n",
    "                    # Update counts (implementation depends on your LDA model)\n",
    "                    # lda_model.update_counts(tokenized_text, i, sampled_topic)\n",
    "\n",
    "                    # Replace the original word\n",
    "                    tokenized_text[i] = nearest_word\n",
    "                \n",
    "    print(\"\\nFinal Sampled Text:\")\n",
    "    # Convert elements in tokenized_text to strings before joining\n",
    "    tokenized_text_str = [str(token) for token in tokenized_text]\n",
    "    return ' '.join(tokenized_text_str)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"software digital technology is outstanding\"\n",
    "sampled_text = gibbs_sampling(input_text, lda_model, word2vec_model, nn, lambda_=0.5)\n",
    "print(sampled_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d54b53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Topic Distributions: [(0, 0.03335175), (1, 0.03430472), (2, 0.5756946), (3, 0.32280558), (4, 0.03384328)]\n",
      "Sampled Topic: 2\n",
      "Similar words to 'software':\n",
      "computer: 0.9748510122299194\n",
      "\n",
      "Most similar word: computer (Similarity: 0.9748510122299194)\n",
      "Most Similar Word: computer\n",
      "Sampled Topic: 1\n",
      "Similar words to 'digital':\n",
      "recorders: 0.9725573658943176\n",
      "\n",
      "Most similar word: recorders (Similarity: 0.9725573658943176)\n",
      "Most Similar Word: recorders\n",
      "Sampled Topic: 4\n",
      "Similar words to 'technology':\n",
      "search: 0.9623100161552429\n",
      "\n",
      "Most similar word: search (Similarity: 0.9623100161552429)\n",
      "Most Similar Word: search\n",
      "\n",
      "Final Sampled Text:\n",
      "computer recorders search is outstanding\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import LdaModel, Word2Vec\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming you have loaded your LDA and Word2Vec models\n",
    "lda_model = train_lda_model\n",
    "word2vec_model = Word2Vec.load('bbc.model')\n",
    "\n",
    "# Initialize Nearest Neighbors\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "nn.fit(word2vec_model.wv.vectors)\n",
    "\n",
    "def find_similar_words(input_word, model, topn=5):\n",
    "    \"\"\"\n",
    "    Find and print similar words to the input word using a pre-trained Word2Vec model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_word (str): The input word for which to find similar words.\n",
    "    - model (Word2Vec): The pre-trained Word2Vec model.\n",
    "    - topn (int): Number of similar words to retrieve (default is 5).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find similar words to the input word\n",
    "        similar_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Similar words to '{input_word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"{word}: {similarity}\")\n",
    "\n",
    "        # Find the most similar word and its similarity score\n",
    "        most_similar_word, max_similarity = max(similar_words, key=lambda x: x[1])\n",
    "        print(f\"\\nMost similar word: {most_similar_word} (Similarity: {max_similarity})\")\n",
    "\n",
    "        return most_similar_word\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Word '{input_word}' not found in the vocabulary.\")\n",
    "        return None\n",
    "\n",
    "def gibbs_sampling(text, lda_model, word2vec_model, nn, lambda_, num_iterations=1):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"Iteration {iteration+1}:\")\n",
    "\n",
    "        # Estimate topic distributions\n",
    "        topic_distributions = lda_model.get_document_topics(\n",
    "            corpora.Dictionary([tokenized_text]).doc2bow(tokenized_text)\n",
    "        )\n",
    "        print(\"Topic Distributions:\", topic_distributions)\n",
    "\n",
    "        # Extract probabilities and normalize\n",
    "        probabilities = np.array([topic[1] for topic in topic_distributions])\n",
    "        normalized_probs = probabilities / np.sum(probabilities)\n",
    "\n",
    "        for i, word in enumerate(tokenized_text):\n",
    "            if word in word2vec_model.wv.key_to_index and random.random() < lambda_:\n",
    "                # Sample a topic\n",
    "                sampled_topic = np.random.choice(num_topics, p=normalized_probs)\n",
    "                print(f\"Sampled Topic: {sampled_topic}\")\n",
    "\n",
    "                # Sample a word vector\n",
    "                p = [prob for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = [prob / np.sum(p) for _, prob in lda_model.get_topic_terms(sampled_topic, topn=len(word2vec_model.wv))]\n",
    "                p = np.asarray(p).astype('float64')\n",
    "                p = p / np.sum(p)\n",
    "                sampled_word_vector = word2vec_model.wv[np.random.choice(word2vec_model.wv.index_to_key, p=p)]\n",
    "\n",
    "                # Find the most similar word using the find_similar_words function\n",
    "                most_similar_word = find_similar_words(word, word2vec_model, topn=1)\n",
    "                print(f\"Most Similar Word: {most_similar_word}\")\n",
    "\n",
    "                if most_similar_word:\n",
    "                    # Replace the original word\n",
    "                    tokenized_text[i] = most_similar_word\n",
    "\n",
    "    print(\"\\nFinal Sampled Text:\")\n",
    "    # Convert elements in tokenized_text to strings before joining\n",
    "    tokenized_text_str = [str(token) for token in tokenized_text]\n",
    "    return ' '.join(tokenized_text_str)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"software digital technology is outstanding\"\n",
    "sampled_text = gibbs_sampling(input_text, lda_model, word2vec_model, nn, lambda_=0.5)\n",
    "print(sampled_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975cf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
